{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "mrpv4tMjvNkB",
        "outputId": "c5d288c5-ac50-484d-af23-88b061f8acad"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# 2. Count the number of tokens\\nprint(\"Number of tokens:\", len(doc))\\n\\n# 3. Count the number of sentences\\nsentences = list(doc.sents)\\nprint(\"Number of sentences:\", len(sentences))\\n\\n# 4. Print the second sentence\\nsecond_sentence = sentences[1]\\nprint(\"Second sentence:\", second_sentence.text)\\n\\n# 5. Print text, POS tag, dep tag, and lemma for each token in the second sentence\\nprint(\"\\nToken details:\")\\nfor token in second_sentence:\\n    print(f\"{token.text:<15} POS: {token.pos_:<10} DEP: {token.dep_:<10} Lemma: {token.lemma_}\")\\n\\n# 6. Create a matcher to find \"swimming vigorously\"\\nmatcher = Matcher(nlp.vocab)\\npattern = [\\n    {\"LOWER\": \"swimming\"},\\n    {\"LOWER\": \"vigorously\"}\\n]\\nmatcher.add(\"Swimming\", [pattern])\\n\\n# 7. Print surrounding text for each match\\nprint(\"\\nMatches and surrounding text:\")\\nmatches = matcher(doc)\\nfor match_id, start, end in matches:\\n    span = doc[start:end]\\n    start_context = max(start - 5, 0)\\n    end_context = min(end + 5, len(doc))\\n    context = doc[start_context:end_context]\\n    print(f\"...{context.text}...\")'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import spacy\n",
        "from spacy.matcher import Matcher\n",
        "\n",
        "# 1. Load spaCy model and create a Doc object from the file\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "with open(\"owlcreek.txt\", \"r\", encoding=\"utf-8\") as file:\n",
        "    text = file.read()\n",
        "doc = nlp(text)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Count the number of tokens\n",
        "print(\"Number of tokens:\", len(doc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jzgRRLDAv6t0",
        "outputId": "7146b118-ec6e-44bc-b21b-cd84324f69ca"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of tokens: 4835\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Count the number of sentences\n",
        "sentences = list(doc.sents)\n",
        "print(\"Number of sentences:\", len(sentences))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kt4g-2r0wBHi",
        "outputId": "7d5c1db2-f15b-49e7-b41f-659ed0f02ef0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of sentences: 204\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Print the second sentence\n",
        "second_sentence = sentences[1]\n",
        "print(\"Second sentence:\", second_sentence.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u_qZKPC6wElL",
        "outputId": "ea703dec-aa59-4d18-c48f-3fc4dd7f643e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Second sentence: The man's hands were behind\n",
            "his back, the wrists bound with a cord.  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Print text, POS tag, dep tag, and lemma for each token in the second sentence\n",
        "print(\"\\nToken details:\")\n",
        "for token in second_sentence:\n",
        "    print(f\"{token.text:<15} POS: {token.pos_:<10} DEP: {token.dep_:<10} Lemma: {token.lemma_}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EXVnjn-4wHDp",
        "outputId": "d26e8639-e4ee-4ba4-c8f7-db241460491a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Token details:\n",
            "The             POS: DET        DEP: det        Lemma: the\n",
            "man             POS: NOUN       DEP: poss       Lemma: man\n",
            "'s              POS: PART       DEP: case       Lemma: 's\n",
            "hands           POS: NOUN       DEP: nsubj      Lemma: hand\n",
            "were            POS: AUX        DEP: ROOT       Lemma: be\n",
            "behind          POS: ADP        DEP: prep       Lemma: behind\n",
            "\n",
            "               POS: SPACE      DEP: dep        Lemma: \n",
            "\n",
            "his             POS: PRON       DEP: poss       Lemma: his\n",
            "back            POS: NOUN       DEP: pobj       Lemma: back\n",
            ",               POS: PUNCT      DEP: punct      Lemma: ,\n",
            "the             POS: DET        DEP: det        Lemma: the\n",
            "wrists          POS: NOUN       DEP: appos      Lemma: wrist\n",
            "bound           POS: VERB       DEP: acl        Lemma: bind\n",
            "with            POS: ADP        DEP: prep       Lemma: with\n",
            "a               POS: DET        DEP: det        Lemma: a\n",
            "cord            POS: NOUN       DEP: pobj       Lemma: cord\n",
            ".               POS: PUNCT      DEP: punct      Lemma: .\n",
            "                POS: SPACE      DEP: dep        Lemma:  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Create a matcher to find \"swimming vigorously\"\n",
        "matcher = Matcher(nlp.vocab)\n",
        "pattern = [\n",
        "    {\"LOWER\": \"swimming\"},\n",
        "    {\"LOWER\": \"vigorously\"}\n",
        "]\n",
        "matcher.add(\"Swimming\", [pattern])"
      ],
      "metadata": {
        "id": "Z1haw_pgwJ3-"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cfqh7IRSwMmH"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GrirEXLDwRfg"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fex78K5twkYD"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Print surrounding text for each match\n",
        "\n",
        "import re\n",
        "# Find all \"swimming vigorously\" (case-insensitive, flexible spacing)\n",
        "matches = re.finditer(r\"\\bswimming\\b[\\s,;:\\-â€”]*\\bvigorously\\b\", text, re.IGNORECASE)\n",
        "\n",
        "# Go through each match\n",
        "for i, m in enumerate(matches, start=1):\n",
        "    span = doc.char_span(m.start(), m.end(), alignment_mode=\"expand\")\n",
        "    print(f\"\\nMatch {i}: \")\n",
        "\n",
        "    # Show nearby context (10 tokens before and after)\n",
        "    start = max(span.start - 10, 0)\n",
        "    end = min(span.end + 10, len(doc))\n",
        "    print(\"Context:\\n\", doc[start:end].text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zl3wHVSlxKPS",
        "outputId": "7452c67c-a28c-48fc-98ae-71a19095b385"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Match 1: \n",
            "Context:\n",
            "  By diving I could evade the bullets and, swimming\n",
            "vigorously, reach the bank, take to the woods and\n",
            "\n",
            "Match 2: \n",
            "Context:\n",
            " saw all this over his shoulder; he was now swimming\n",
            "vigorously with the current.  His brain was as energetic\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZmmTmrs0xtIQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}